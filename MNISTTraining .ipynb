{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNISTTraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmZHDWSKVb64"
      },
      "source": [
        "# Libraries used\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   Deep Learning: torch, torchvision \r\n",
        "*   Data Manipulation: numpy, pandas, sklearn\r\n",
        "*   Plotting: PIL, seaborn, matplotlib\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi5Fun__Fr-1"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "from torchvision import datasets, transforms, models\r\n",
        "import seaborn as sb\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAy0H9pMNKbr"
      },
      "source": [
        "# LeNet5 architecture for Transfer Learning\r\n",
        "\r\n",
        "LeNet5 is a Convolutional Neural Network architecture made by Yann Lecun on 1998 using specifically the MNIST handwritten 28x28 images dataset which consists of 60,000 labeled images for trainning and 10,000 images for testing the model.\r\n",
        "\r\n",
        "For this Kaggle competition, the dataset given has images of 28x28 but the architecture used recieves data of dimension 32x32, this is why an extra adaptative-average-pooling layer was added before the flattering process for getting the required dimensions to fit the first fully-connected layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECZXh4rJGzQa"
      },
      "source": [
        "class LeNet5(torch.nn.Module):          \r\n",
        "     \r\n",
        "    def __init__(self):     \r\n",
        "        super(LeNet5, self).__init__()\r\n",
        "        # Convolution (In LeNet-5, 32x32 images are given as input. Hence padding of 2 is done below)\r\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)\r\n",
        "        # Max-pooling\r\n",
        "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\r\n",
        "        # Convolution\r\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\r\n",
        "        # Max-pooling\r\n",
        "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2) \r\n",
        "        # Adaptative-average-pooling\r\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((5, 5))\r\n",
        "        \r\n",
        "        # Fully connected layer\r\n",
        "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\r\n",
        "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\r\n",
        "        self.fc3 = torch.nn.Linear(84, 10)        # convert matrix with 84 features to a matrix of 10 features (columns)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        # convolve, then perform ReLU non-linearity\r\n",
        "        x = torch.nn.functional.relu(self.conv1(x))  \r\n",
        "        # max-pooling with 2x2 grid \r\n",
        "        x = self.max_pool_1(x) \r\n",
        "        # convolve, then perform ReLU non-linearity\r\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\r\n",
        "        # max-pooling with 2x2 grid\r\n",
        "        x = self.max_pool_2(x) \r\n",
        "        # avg-pooling\r\n",
        "        x = self.avgpool(x)\r\n",
        "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\r\n",
        "        x = x.view(-1, 16*5*5)\r\n",
        "        # FC-1, then perform ReLU non-linearity\r\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\r\n",
        "        # FC-2, then perform ReLU non-linearity\r\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\r\n",
        "        # FC-3\r\n",
        "        x = self.fc3(x)\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5dENycSuQj2"
      },
      "source": [
        "# Data Preprocessing\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbcJJmHqHGEU"
      },
      "source": [
        "# Reading the data from the CSVs given.\r\n",
        "\r\n",
        "train = pd.read_csv('train.csv',dtype = np.float32)\r\n",
        "test = pd.read_csv('test.csv',dtype = np.float32)\r\n",
        "\r\n",
        "\r\n",
        "# Extracting the values for the training set\r\n",
        "\r\n",
        "X = train.drop(\"label\", axis=1).values\r\n",
        "y = train[\"label\"].values\r\n",
        "\r\n",
        "\r\n",
        "# Normalizing data\r\n",
        "\r\n",
        "X = X/255.0\r\n",
        "\r\n",
        "X_test = test.values/255.0\r\n",
        "\r\n",
        "\r\n",
        "# Train - Valid split\r\n",
        "\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.2,random_state = 42)\r\n",
        "\r\n",
        "\r\n",
        "# Converting data to tensor\r\n",
        "\r\n",
        "X_train = torch.from_numpy(X_train)\r\n",
        "\r\n",
        "X_val = torch.from_numpy(X_val)\r\n",
        "\r\n",
        "X_test= torch.from_numpy(X_test)\r\n",
        "\r\n",
        "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\r\n",
        "\r\n",
        "y_val = torch.from_numpy(y_val).type(torch.LongTensor)\r\n",
        "\r\n",
        "\r\n",
        "# Unsqueezing the data for getting the gray channel\r\n",
        "\r\n",
        "X_train = X_train.unsqueeze(1)\r\n",
        "\r\n",
        "X_val = X_val.unsqueeze(1)\r\n",
        "\r\n",
        "X_test = X_test.unsqueeze(1)\r\n",
        "\r\n",
        "\r\n",
        "# Reshaping data for getting the right dimensions for LeNet5 input\r\n",
        "\r\n",
        "X_train = X_train.reshape(-1,1,28,28)\r\n",
        "\r\n",
        "X_val = X_val.reshape(-1,1,28,28)\r\n",
        "\r\n",
        "X_test = X_test.reshape(-1,1,28,28)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Converting all to TensorDatasets\r\n",
        "\r\n",
        "train_tensor = torch.utils.data.TensorDataset(X_train, y_train)\r\n",
        "\r\n",
        "validation_tensor = torch.utils.data.TensorDataset(X_val, y_val)\r\n",
        "\r\n",
        "test_tensor = torch.utils.data.TensorDataset(X_test)\r\n",
        "\r\n",
        "\r\n",
        "# Final DataLoaders\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_tensor, \r\n",
        "                                           batch_size = 64,\r\n",
        "                                           shuffle = True)\r\n",
        "\r\n",
        "validation_loader = torch.utils.data.DataLoader(validation_tensor, \r\n",
        "                                           batch_size = 64,\r\n",
        "                                           shuffle = False)\r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(test_tensor, \r\n",
        "                                          batch_size = 64,\r\n",
        "                                          shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6pAJxXdzJz9"
      },
      "source": [
        "# Model Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tqDIBPeF5IG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ec8f09-3e44-4ea1-9f91-45a13d3e4757"
      },
      "source": [
        "model = LeNet5()\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n",
        "\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "model.to(device)\r\n",
        "running_loss = 0\r\n",
        "steps = 0\r\n",
        "\r\n",
        "print('Training Started!')\r\n",
        "\r\n",
        "for e in range(epochs):\r\n",
        "\r\n",
        "  print('Epoch number: ', e+1)\r\n",
        "\r\n",
        "  for inputs, labels in train_loader:\r\n",
        "\r\n",
        "    #Training Loop\r\n",
        "\r\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "    optimizer.zero_grad()\r\n",
        "    outputs = model.forward(inputs)\r\n",
        "    loss = criterion(outputs, labels)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    running_loss += loss.item()\r\n",
        "\r\n",
        "    steps += 1\r\n",
        "\r\n",
        "\r\n",
        "    # Validating after 3,200 sample images\r\n",
        "\r\n",
        "    if steps == 50:\r\n",
        "        model.eval()\r\n",
        "        accuracy = 0\r\n",
        "        valid_loss = 0\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "\r\n",
        "            for inputs, labels in validation_loader:\r\n",
        "\r\n",
        "                #Validation Loop\r\n",
        "\r\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "                outputs = model.forward(inputs)\r\n",
        "                top_p, top_class = outputs.topk(1, dim=1)\r\n",
        "                loss_valid = criterion(outputs, labels)\r\n",
        "                valid_loss += loss_valid.item()\r\n",
        "\r\n",
        "                equals = top_class == labels.view(*top_class.shape)\r\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\r\n",
        "\r\n",
        "            print(\r\n",
        "              f\"Train loss: {running_loss/steps:.3f}.. \"\r\n",
        "              f\"Valid loss: {valid_loss/len(validation_loader):.3f}.. \"\r\n",
        "              f\"Valid accuracy: {accuracy/len(validation_loader):.3f}\")\r\n",
        "\r\n",
        "        running_loss = 0 \r\n",
        "        steps = 0        \r\n",
        "        model.train()\r\n",
        "\r\n",
        "\r\n",
        "print('Training finished!')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Started!\n",
            "Epoch number:  1\n",
            "Train loss: 1.725.. Valid loss: 0.833.. Valid accuracy: 0.729\n",
            "Train loss: 0.565.. Valid loss: 0.407.. Valid accuracy: 0.876\n",
            "Train loss: 0.350.. Valid loss: 0.306.. Valid accuracy: 0.905\n",
            "Train loss: 0.275.. Valid loss: 0.259.. Valid accuracy: 0.922\n",
            "Train loss: 0.235.. Valid loss: 0.218.. Valid accuracy: 0.936\n",
            "Train loss: 0.175.. Valid loss: 0.194.. Valid accuracy: 0.942\n",
            "Train loss: 0.196.. Valid loss: 0.186.. Valid accuracy: 0.942\n",
            "Train loss: 0.139.. Valid loss: 0.142.. Valid accuracy: 0.956\n",
            "Train loss: 0.127.. Valid loss: 0.144.. Valid accuracy: 0.957\n",
            "Train loss: 0.129.. Valid loss: 0.150.. Valid accuracy: 0.955\n",
            "Epoch number:  2\n",
            "Train loss: 0.105.. Valid loss: 0.128.. Valid accuracy: 0.960\n",
            "Train loss: 0.101.. Valid loss: 0.128.. Valid accuracy: 0.959\n",
            "Train loss: 0.097.. Valid loss: 0.110.. Valid accuracy: 0.967\n",
            "Train loss: 0.100.. Valid loss: 0.100.. Valid accuracy: 0.969\n",
            "Train loss: 0.098.. Valid loss: 0.111.. Valid accuracy: 0.967\n",
            "Train loss: 0.102.. Valid loss: 0.097.. Valid accuracy: 0.970\n",
            "Train loss: 0.088.. Valid loss: 0.104.. Valid accuracy: 0.968\n",
            "Train loss: 0.097.. Valid loss: 0.091.. Valid accuracy: 0.972\n",
            "Train loss: 0.080.. Valid loss: 0.090.. Valid accuracy: 0.971\n",
            "Train loss: 0.084.. Valid loss: 0.074.. Valid accuracy: 0.977\n",
            "Train loss: 0.086.. Valid loss: 0.087.. Valid accuracy: 0.973\n",
            "Epoch number:  3\n",
            "Train loss: 0.071.. Valid loss: 0.076.. Valid accuracy: 0.978\n",
            "Train loss: 0.070.. Valid loss: 0.070.. Valid accuracy: 0.978\n",
            "Train loss: 0.070.. Valid loss: 0.084.. Valid accuracy: 0.974\n",
            "Train loss: 0.060.. Valid loss: 0.077.. Valid accuracy: 0.975\n",
            "Train loss: 0.071.. Valid loss: 0.067.. Valid accuracy: 0.978\n",
            "Train loss: 0.062.. Valid loss: 0.075.. Valid accuracy: 0.977\n",
            "Train loss: 0.056.. Valid loss: 0.068.. Valid accuracy: 0.979\n",
            "Train loss: 0.084.. Valid loss: 0.094.. Valid accuracy: 0.970\n",
            "Train loss: 0.060.. Valid loss: 0.070.. Valid accuracy: 0.978\n",
            "Train loss: 0.054.. Valid loss: 0.081.. Valid accuracy: 0.976\n",
            "Epoch number:  4\n",
            "Train loss: 0.053.. Valid loss: 0.062.. Valid accuracy: 0.980\n",
            "Train loss: 0.051.. Valid loss: 0.060.. Valid accuracy: 0.980\n",
            "Train loss: 0.051.. Valid loss: 0.069.. Valid accuracy: 0.980\n",
            "Train loss: 0.056.. Valid loss: 0.079.. Valid accuracy: 0.975\n",
            "Train loss: 0.057.. Valid loss: 0.059.. Valid accuracy: 0.981\n",
            "Train loss: 0.050.. Valid loss: 0.062.. Valid accuracy: 0.981\n",
            "Train loss: 0.053.. Valid loss: 0.064.. Valid accuracy: 0.981\n",
            "Train loss: 0.053.. Valid loss: 0.070.. Valid accuracy: 0.978\n",
            "Train loss: 0.040.. Valid loss: 0.055.. Valid accuracy: 0.983\n",
            "Train loss: 0.065.. Valid loss: 0.053.. Valid accuracy: 0.983\n",
            "Train loss: 0.050.. Valid loss: 0.059.. Valid accuracy: 0.982\n",
            "Epoch number:  5\n",
            "Train loss: 0.033.. Valid loss: 0.053.. Valid accuracy: 0.982\n",
            "Train loss: 0.036.. Valid loss: 0.058.. Valid accuracy: 0.981\n",
            "Train loss: 0.048.. Valid loss: 0.069.. Valid accuracy: 0.979\n",
            "Train loss: 0.037.. Valid loss: 0.062.. Valid accuracy: 0.980\n",
            "Train loss: 0.048.. Valid loss: 0.056.. Valid accuracy: 0.981\n",
            "Train loss: 0.050.. Valid loss: 0.057.. Valid accuracy: 0.981\n",
            "Train loss: 0.046.. Valid loss: 0.048.. Valid accuracy: 0.984\n",
            "Train loss: 0.035.. Valid loss: 0.054.. Valid accuracy: 0.982\n",
            "Train loss: 0.041.. Valid loss: 0.055.. Valid accuracy: 0.981\n",
            "Train loss: 0.043.. Valid loss: 0.055.. Valid accuracy: 0.982\n",
            "Epoch number:  6\n",
            "Train loss: 0.040.. Valid loss: 0.057.. Valid accuracy: 0.982\n",
            "Train loss: 0.030.. Valid loss: 0.058.. Valid accuracy: 0.982\n",
            "Train loss: 0.036.. Valid loss: 0.049.. Valid accuracy: 0.985\n",
            "Train loss: 0.030.. Valid loss: 0.048.. Valid accuracy: 0.986\n",
            "Train loss: 0.036.. Valid loss: 0.056.. Valid accuracy: 0.982\n",
            "Train loss: 0.035.. Valid loss: 0.050.. Valid accuracy: 0.983\n",
            "Train loss: 0.036.. Valid loss: 0.049.. Valid accuracy: 0.984\n",
            "Train loss: 0.038.. Valid loss: 0.046.. Valid accuracy: 0.985\n",
            "Train loss: 0.030.. Valid loss: 0.055.. Valid accuracy: 0.984\n",
            "Train loss: 0.051.. Valid loss: 0.069.. Valid accuracy: 0.976\n",
            "Train loss: 0.037.. Valid loss: 0.047.. Valid accuracy: 0.985\n",
            "Epoch number:  7\n",
            "Train loss: 0.029.. Valid loss: 0.052.. Valid accuracy: 0.983\n",
            "Train loss: 0.028.. Valid loss: 0.056.. Valid accuracy: 0.982\n",
            "Train loss: 0.026.. Valid loss: 0.047.. Valid accuracy: 0.986\n",
            "Train loss: 0.028.. Valid loss: 0.060.. Valid accuracy: 0.982\n",
            "Train loss: 0.036.. Valid loss: 0.050.. Valid accuracy: 0.984\n",
            "Train loss: 0.030.. Valid loss: 0.046.. Valid accuracy: 0.986\n",
            "Train loss: 0.034.. Valid loss: 0.047.. Valid accuracy: 0.985\n",
            "Train loss: 0.041.. Valid loss: 0.053.. Valid accuracy: 0.982\n",
            "Train loss: 0.031.. Valid loss: 0.045.. Valid accuracy: 0.985\n",
            "Train loss: 0.023.. Valid loss: 0.044.. Valid accuracy: 0.987\n",
            "Epoch number:  8\n",
            "Train loss: 0.022.. Valid loss: 0.042.. Valid accuracy: 0.987\n",
            "Train loss: 0.016.. Valid loss: 0.054.. Valid accuracy: 0.984\n",
            "Train loss: 0.023.. Valid loss: 0.047.. Valid accuracy: 0.986\n",
            "Train loss: 0.019.. Valid loss: 0.050.. Valid accuracy: 0.985\n",
            "Train loss: 0.034.. Valid loss: 0.044.. Valid accuracy: 0.987\n",
            "Train loss: 0.028.. Valid loss: 0.044.. Valid accuracy: 0.986\n",
            "Train loss: 0.031.. Valid loss: 0.047.. Valid accuracy: 0.984\n",
            "Train loss: 0.028.. Valid loss: 0.042.. Valid accuracy: 0.987\n",
            "Train loss: 0.027.. Valid loss: 0.057.. Valid accuracy: 0.983\n",
            "Train loss: 0.033.. Valid loss: 0.042.. Valid accuracy: 0.987\n",
            "Train loss: 0.022.. Valid loss: 0.052.. Valid accuracy: 0.984\n",
            "Epoch number:  9\n",
            "Train loss: 0.024.. Valid loss: 0.044.. Valid accuracy: 0.985\n",
            "Train loss: 0.020.. Valid loss: 0.045.. Valid accuracy: 0.986\n",
            "Train loss: 0.018.. Valid loss: 0.044.. Valid accuracy: 0.985\n",
            "Train loss: 0.011.. Valid loss: 0.052.. Valid accuracy: 0.985\n",
            "Train loss: 0.032.. Valid loss: 0.048.. Valid accuracy: 0.985\n",
            "Train loss: 0.027.. Valid loss: 0.045.. Valid accuracy: 0.986\n",
            "Train loss: 0.018.. Valid loss: 0.044.. Valid accuracy: 0.987\n",
            "Train loss: 0.022.. Valid loss: 0.050.. Valid accuracy: 0.985\n",
            "Train loss: 0.019.. Valid loss: 0.052.. Valid accuracy: 0.984\n",
            "Train loss: 0.016.. Valid loss: 0.053.. Valid accuracy: 0.985\n",
            "Epoch number:  10\n",
            "Train loss: 0.017.. Valid loss: 0.042.. Valid accuracy: 0.987\n",
            "Train loss: 0.016.. Valid loss: 0.044.. Valid accuracy: 0.986\n",
            "Train loss: 0.010.. Valid loss: 0.051.. Valid accuracy: 0.983\n",
            "Train loss: 0.017.. Valid loss: 0.055.. Valid accuracy: 0.984\n",
            "Train loss: 0.021.. Valid loss: 0.048.. Valid accuracy: 0.986\n",
            "Train loss: 0.016.. Valid loss: 0.043.. Valid accuracy: 0.986\n",
            "Train loss: 0.025.. Valid loss: 0.050.. Valid accuracy: 0.984\n",
            "Train loss: 0.019.. Valid loss: 0.058.. Valid accuracy: 0.984\n",
            "Train loss: 0.026.. Valid loss: 0.043.. Valid accuracy: 0.988\n",
            "Train loss: 0.024.. Valid loss: 0.044.. Valid accuracy: 0.986\n",
            "Train loss: 0.021.. Valid loss: 0.051.. Valid accuracy: 0.984\n",
            "Training finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWVCgWTgF7Bu"
      },
      "source": [
        "# Submission results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66nnnhat4gsM"
      },
      "source": [
        "model.eval()\r\n",
        "outputsTensor = torch.LongTensor()\r\n",
        "with torch.no_grad():\r\n",
        "    for inputs in test_loader:\r\n",
        "        #Test Loop\r\n",
        "        inputs = inputs[0].to(device)\r\n",
        "        outputs = model.forward(inputs)\r\n",
        "        top_p, top_class = outputs.topk(1, dim=1)\r\n",
        "        outputsTensor = torch.cat((outputsTensor, top_class), dim = 0)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFd-x22lF6VH"
      },
      "source": [
        "# Submission format arrangement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwIxaLD73iEn"
      },
      "source": [
        "outputsTensor = outputsTensor.tolist()\r\n",
        "lista = []\r\n",
        "for i in outputsTensor:\r\n",
        "  i = str(i)\r\n",
        "  lista.append(i[1])\r\n",
        "\r\n",
        "Labels = pd.Series(lista)\r\n",
        "ImageID = pd.Series(np.arange(1,28001))\r\n",
        "\r\n",
        "submission = pd.DataFrame({\"ImageId\": ImageID,\r\n",
        "                          \"Labels\": Labels})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWMK0Wb0IcRj"
      },
      "source": [
        "# CSV and Model saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z57-lxQ_oMus"
      },
      "source": [
        "submission.to_csv(\"submission.csv\", index = False)\r\n",
        "torch.save(model, 'mnist.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}